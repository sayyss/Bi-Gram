
### Takes in x, and y, uses CLE formula to total loss
### returns total loss(a scalar tensor)
### x: tensor([])
### y: tensor([])
def CLE(x,y):
    return


### X: tensor([])
### returns a tensor same size x as softmax function applied
def softmax(x):
    return

### returns one_hot encoded values 
### x: tensor([])
### n: number of classes to encode into
def one_hot_encode(x, n):
    return


### Takes in x: tensor([])
### Returns a tensor of the same size as x with tanh activation applied
def tanh(x):
    return




